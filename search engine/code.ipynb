{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading collection 'popular'\n",
      "[nltk_data]    | \n",
      "[nltk_data]    | Downloading package cmudict to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package cmudict is already up-to-date!\n",
      "[nltk_data]    | Downloading package gazetteers to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package gazetteers is already up-to-date!\n",
      "[nltk_data]    | Downloading package genesis to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package genesis is already up-to-date!\n",
      "[nltk_data]    | Downloading package gutenberg to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package gutenberg is already up-to-date!\n",
      "[nltk_data]    | Downloading package inaugural to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package inaugural is already up-to-date!\n",
      "[nltk_data]    | Downloading package movie_reviews to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package movie_reviews is already up-to-date!\n",
      "[nltk_data]    | Downloading package names to /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package names is already up-to-date!\n",
      "[nltk_data]    | Downloading package shakespeare to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package shakespeare is already up-to-date!\n",
      "[nltk_data]    | Downloading package stopwords to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package stopwords is already up-to-date!\n",
      "[nltk_data]    | Downloading package treebank to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package treebank is already up-to-date!\n",
      "[nltk_data]    | Downloading package twitter_samples to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package twitter_samples is already up-to-date!\n",
      "[nltk_data]    | Downloading package omw to /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package omw is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package wordnet is already up-to-date!\n",
      "[nltk_data]    | Downloading package wordnet_ic to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package wordnet_ic is already up-to-date!\n",
      "[nltk_data]    | Downloading package words to /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package words is already up-to-date!\n",
      "[nltk_data]    | Downloading package maxent_ne_chunker to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package maxent_ne_chunker is already up-to-date!\n",
      "[nltk_data]    | Downloading package punkt to /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package punkt is already up-to-date!\n",
      "[nltk_data]    | Downloading package snowball_data to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package snowball_data is already up-to-date!\n",
      "[nltk_data]    | Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]    |     /home/ashar/nltk_data...\n",
      "[nltk_data]    |   Package averaged_perceptron_tagger is already up-\n",
      "[nltk_data]    |       to-date!\n",
      "[nltk_data]    | \n",
      "[nltk_data]  Done downloading collection popular\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: num2words in /home/ashar/anaconda3/envs/beyond_data/lib/python3.7/site-packages (0.5.10)\r\n",
      "Requirement already satisfied: docopt>=0.6.2 in /home/ashar/anaconda3/envs/beyond_data/lib/python3.7/site-packages (from num2words) (0.6.2)\r\n"
     ]
    }
   ],
   "source": [
    "#For preprocessing of data- preprocessing pipeline\n",
    "\n",
    "import nltk\n",
    "import os\n",
    "import string\n",
    "import numpy as np\n",
    "import copy\n",
    "import pandas as pd\n",
    "import pickle\n",
    "import re\n",
    "import math\n",
    "import num2words\n",
    "\n",
    "nltk.download(\"popular\")\n",
    "!pip install num2words\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.stem import PorterStemmer\n",
    "from collections import Counter\n",
    "from num2words import num2words\n",
    "\n",
    "\n",
    "\n",
    "def remove_stop_words(data):\n",
    "    stop_words = stopwords.words('french')\n",
    "    words = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in words:\n",
    "        if w not in stop_words and len(w) > 1:\n",
    "            new_text = new_text + \" \" + w\n",
    "    return new_text\n",
    "\n",
    "def stemming(data):\n",
    "    stemmer= PorterStemmer()\n",
    "    \n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        new_text = new_text + \" \" + stemmer.stem(w)\n",
    "    return new_text\n",
    "def convert_numbers(data):\n",
    "    tokens = word_tokenize(str(data))\n",
    "    new_text = \"\"\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            w = num2words(int(w))\n",
    "        except:\n",
    "            a = 0\n",
    "        new_text = new_text + \" \" + w\n",
    "    new_text = np.char.replace(new_text, \"-\", \" \")\n",
    "    return new_text\n",
    "\n",
    "def convert_lower_case(data):\n",
    "    return np.char.lower(str(data))\n",
    "\n",
    "def remove_punctuation(data):\n",
    "    symbols = \"!\\\"#$%&()*+-./:;<=>?@[\\]^_`{|}~\\n\"\n",
    "    for i in range(len(symbols)):\n",
    "        data = np.char.replace(data, symbols[i], ' ')\n",
    "        data = np.char.replace(data, \"  \", \" \")\n",
    "    data = np.char.replace(data, ',', '')\n",
    "    return data\n",
    "\n",
    "def remove_apostrophe(data):\n",
    "    return np.char.replace(data, \"'\", \"\")\n",
    "\n",
    "\n",
    "def preprocess(data):\n",
    "    data = convert_lower_case(data)\n",
    "    data = remove_punctuation(data) #remove comma seperately\n",
    "    data = remove_apostrophe(data)\n",
    "    data = remove_stop_words(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data)\n",
    "    data = remove_punctuation(data)\n",
    "    data = convert_numbers(data)\n",
    "    data = stemming(data) #needed again as we need to stem the words\n",
    "    data = remove_punctuation(data) #needed again as num2word is giving few hypens and commas fourty-one\n",
    "    data = remove_stop_words(data) #needed again as num2word is giving stop words 101 - one hundred and one\n",
    "    return data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     Nom du tiers_First name  \\\n",
      "0                         Ya   \n",
      "1                         Ma   \n",
      "2                         Ma   \n",
      "3                         Da   \n",
      "4                         Da   \n",
      "...                      ...   \n",
      "7528                      ST   \n",
      "7529                      LU   \n",
      "7530                      DA   \n",
      "7531                      SO   \n",
      "7532                      CO   \n",
      "\n",
      "                    Poste du tiers_ Title of first name  \\\n",
      "0                  Consultant IT indépendant, PMP, ITIL   \n",
      "1                        Consultant indépendant, we IT.   \n",
      "2                                  Freelance IT Manager   \n",
      "3                               Consultant IT Freelance   \n",
      "4     Finance Consultant in Digital Transformation a...   \n",
      "...                                                 ...   \n",
      "7528  Responsable Informatique - Technologies Libres...   \n",
      "7529                                Coach professionnel   \n",
      "7530         Business Analyst / Consultant informatique   \n",
      "7531              Directeur des Systèmes d'Informations   \n",
      "7532                 consultant RGPD DPO GDPR freelance   \n",
      "\n",
      "     Compétences(details about title of first name to take into account)  \\\n",
      "0     PMP|ITIL|Scrum|Software Development|Software P...                    \n",
      "1     ITIL|Gestion de projet|Stratégie IT|Gouvernanc...                    \n",
      "2     Gestion de projet|Project Management|SQL|Softw...                    \n",
      "3     Java|Agile Methodologies|Scala|Méthodes agiles...                    \n",
      "4     Management|Marketing|Anglais|Gestion de projet...                    \n",
      "...                                                 ...                    \n",
      "7528  Concevoir le schéma directeur à partir des ori...                    \n",
      "7529  Analyser des problèmes techniques|Analyser les...                    \n",
      "7530  Analyser des problèmes techniques|Animer une r...                    \n",
      "7531  Concevoir le schéma directeur à partir des ori...                    \n",
      "7532  Règlement Général européen sur la Protection d...                    \n",
      "\n",
      "                                                   full  \n",
      "0     Consultant IT indépendant, PMP, ITIL  PMP|ITIL...  \n",
      "1     Consultant indépendant, we IT.  ITIL|Gestion d...  \n",
      "2     Freelance IT Manager  Gestion de projet|Projec...  \n",
      "3     Consultant IT Freelance  Java|Agile Methodolog...  \n",
      "4     Finance Consultant in Digital Transformation a...  \n",
      "...                                                 ...  \n",
      "7528  Responsable Informatique - Technologies Libres...  \n",
      "7529  Coach professionnel  Analyser des problèmes te...  \n",
      "7530  Business Analyst / Consultant informatique  An...  \n",
      "7531  Directeur des Systèmes d'Informations  Concevo...  \n",
      "7532  consultant RGPD DPO GDPR freelance  Règlement ...  \n",
      "\n",
      "[7533 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "#Extract all title and description from database\n",
    "\n",
    "fdata=pd.read_csv('/home/ashar/fiverr/search engine/database.csv',encoding = \"ISO-8859-1\")\n",
    "main_data=fdata\n",
    "\n",
    "for index,row in main_data.iterrows():\n",
    "    main_data['full']=main_data['Poste du tiers_ Title of first name']+'  '+main_data['Compétences(details about title of first name to take into account)']\n",
    "print(main_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "text: 0\n",
      "text: 100\n",
      "text: 200\n",
      "text: 300\n",
      "text: 400\n",
      "text: 500\n",
      "text: 600\n",
      "text: 700\n",
      "text: 800\n",
      "text: 900\n",
      "text: 1000\n",
      "text: 1100\n",
      "text: 1200\n",
      "text: 1300\n",
      "text: 1400\n",
      "text: 1500\n",
      "text: 1600\n",
      "text: 1700\n",
      "text: 1800\n",
      "text: 1900\n",
      "text: 2000\n",
      "text: 2100\n",
      "text: 2200\n",
      "text: 2300\n",
      "text: 2400\n",
      "text: 2500\n",
      "text: 2600\n",
      "text: 2700\n",
      "text: 2800\n",
      "text: 2900\n",
      "text: 3000\n",
      "text: 3100\n",
      "text: 3200\n",
      "text: 3300\n",
      "text: 3400\n",
      "text: 3500\n",
      "text: 3600\n",
      "text: 3700\n",
      "text: 3800\n",
      "text: 3900\n",
      "text: 4000\n",
      "text: 4100\n",
      "text: 4200\n",
      "text: 4300\n",
      "text: 4400\n",
      "text: 4500\n",
      "text: 4600\n",
      "text: 4700\n",
      "text: 4800\n",
      "text: 4900\n",
      "text: 5000\n",
      "text: 5100\n",
      "text: 5200\n",
      "text: 5300\n",
      "text: 5400\n",
      "text: 5500\n",
      "text: 5600\n",
      "text: 5700\n",
      "text: 5800\n",
      "text: 5900\n",
      "text: 6000\n",
      "text: 6100\n",
      "text: 6200\n",
      "text: 6300\n",
      "text: 6400\n",
      "text: 6500\n",
      "text: 6600\n",
      "text: 6700\n",
      "text: 6800\n",
      "text: 6900\n",
      "text: 7000\n",
      "text: 7100\n",
      "text: 7200\n",
      "text: 7300\n",
      "text: 7400\n",
      "text: 7500\n",
      "7533\n"
     ]
    }
   ],
   "source": [
    "#Does preprocessing of data with above mentioned preprocess pipeline\n",
    "\n",
    "processed_text=[]\n",
    "i=0\n",
    "for t in main_data['full']:\n",
    "    processed_text.append(word_tokenize(str(preprocess(t))))\n",
    "    if i%100==0:\n",
    "        print(\"text: \"+str(i))\n",
    "    i+=1\n",
    "\n",
    "print(len(processed_text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Extracting vocabulory from every extracted data\n",
    "\n",
    "N=len(processed_text)\n",
    "DF = {}\n",
    "\n",
    "for i in range(N):\n",
    "    tokens = processed_text[i]\n",
    "    for w in tokens:\n",
    "        try:\n",
    "            DF[w].add(i)\n",
    "        except:\n",
    "            DF[w] = {i}\n",
    "for i in DF:\n",
    "    DF[i] = len(DF[i])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "8167"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store vocabolary size\n",
    "total_vocab_size = len(DF)\n",
    "total_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['consult',\n",
       " 'it',\n",
       " 'indépend',\n",
       " 'pmp',\n",
       " 'itil',\n",
       " 'scrum',\n",
       " 'softwar',\n",
       " 'develop',\n",
       " 'project',\n",
       " 'manag',\n",
       " 'développ',\n",
       " 'logiciel',\n",
       " 'chang',\n",
       " 'gestion',\n",
       " 'projet',\n",
       " 'we',\n",
       " 'stratégi',\n",
       " 'gouvern',\n",
       " 'commerci',\n",
       " 'servic']"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#store total vocabolary in list\n",
    "total_vocab = [x for x in DF]\n",
    "total_vocab[:20]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find document frequency of word\n",
    "def doc_freq(word):\n",
    "    c = 0\n",
    "    try:\n",
    "        c = DF[word]\n",
    "    except:\n",
    "        pass\n",
    "    return c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216015"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find tf-idf of every word\n",
    "from collections import Counter\n",
    "doc = 0\n",
    "\n",
    "tf_idf = {}\n",
    "\n",
    "for i in range(N):\n",
    "    \n",
    "    tokens = processed_text[i]\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = np.log((N+1)/(df+1))\n",
    "        \n",
    "        tf_idf[doc, token] = tf*idf\n",
    "\n",
    "    doc += 1\n",
    "    \n",
    "len(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "216015"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#find cosine similarity of sentences\n",
    "def cosine_sim(a, b):\n",
    "    cos_sim = np.dot(a, b)/(np.linalg.norm(a)*np.linalg.norm(b))\n",
    "    return cos_sim\n",
    "\n",
    "len(tf_idf)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0\n",
      "100000\n",
      "200000\n"
     ]
    }
   ],
   "source": [
    "#indexing of words\n",
    "D = np.zeros((N, total_vocab_size))\n",
    "cnt=0\n",
    "for i in tf_idf:\n",
    "    if cnt%100000==0:\n",
    "        print(cnt)\n",
    "    try:\n",
    "        ind = total_vocab.index(i[1])\n",
    "        D[i[0]][ind] = tf_idf[i]\n",
    "    except:\n",
    "        pass\n",
    "    cnt+=1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "#generate vector for words\n",
    "def gen_vector(tokens):\n",
    "\n",
    "    Q = np.zeros((len(total_vocab)))\n",
    "    \n",
    "    counter = Counter(tokens)\n",
    "    words_count = len(tokens)\n",
    "\n",
    "    query_weights = {}\n",
    "    \n",
    "    for token in np.unique(tokens):\n",
    "        \n",
    "        tf = counter[token]/words_count\n",
    "        df = doc_freq(token)\n",
    "        idf = math.log((N+1)/(df+1))\n",
    "\n",
    "        try:\n",
    "            ind = total_vocab.index(token)\n",
    "            Q[ind] = tf*idf\n",
    "        except:\n",
    "            pass\n",
    "    return Q"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "#find cosine similarity\n",
    "\n",
    "def cosine_similarity(k, query):\n",
    "    preprocessed_query = preprocess(query)\n",
    "    tokens = word_tokenize(str(preprocessed_query))\n",
    "    \n",
    "    d_cosines = []\n",
    "    \n",
    "    query_vector = gen_vector(tokens)\n",
    "    \n",
    "    for d in D:\n",
    "        d_cosines.append(cosine_sim(query_vector, d))\n",
    "        \n",
    "    out = np.array(d_cosines).argsort()[-k:][::-1]\n",
    "\n",
    "    return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Empty DataFrame\n",
      "Columns: [query, top_names, top_titles, top_descriptions, score]\n",
      "Index: []\n"
     ]
    }
   ],
   "source": [
    "from gensim import corpora, models, similarities\n",
    "import jieba\n",
    "import re\n",
    "#sentence similarity\n",
    "def isQualified(sentence,sentList,simScore):\n",
    "    maxVal=0\n",
    "    texts = sentList\n",
    "    keyword = sentence\n",
    "    texts = [jieba.lcut(str(text)) for text in texts]\n",
    "    dictionary = corpora.Dictionary(texts)\n",
    "    feature_cnt = len(dictionary.token2id)\n",
    "    corpus = [dictionary.doc2bow(text) for text in texts]\n",
    "    tfidf = models.TfidfModel(corpus) \n",
    "    kw_vector = dictionary.doc2bow(jieba.lcut(keyword))\n",
    "    index = similarities.SparseMatrixSimilarity(tfidf[corpus], num_features = feature_cnt)\n",
    "    sim = index[tfidf[kw_vector]]\n",
    "    \n",
    "    return sim\n",
    "\n",
    "querydata=pd.read_csv('updatedq.csv',encoding = \"ISO-8859-1\")\n",
    "# print(querydata)\n",
    "results=pd.DataFrame(columns=['query','top_names','top_titles','top_descriptions','score'])\n",
    "querycount=0\n",
    "\n",
    "query='Ingénieur Etudes et Développement Backend PHP '#type query here\n",
    "keywords='Stack, java stack, java angular'.split(',') #give coma separated keywords string\n",
    "description='' #give description of query\n",
    "total_query=query+description\n",
    "top_titles=[]\n",
    "top_des=[]\n",
    "top_full=[]\n",
    "top_names=[]\n",
    "extract_titles=[]\n",
    "extract_des=[]\n",
    "extract_names=[]\n",
    "Q = cosine_similarity(50, total_query)\n",
    "#check for keywords\n",
    "for x in range(len(Q)):\n",
    "    title=str(main_data.iloc[Q[x]]['Poste du tiers_ Title of first name'])\n",
    "    full=str(main_data.iloc[Q[x]]['full'])\n",
    "    for word in keywords:\n",
    "        if word in full:   #for title bias change 'full' to 'title', 'full' is for description biasness\n",
    "            top_titles.append(main_data.iloc[Q[x]]['Poste du tiers_ Title of first name'])\n",
    "            top_des.append(main_data.iloc[Q[x]]['Compétences(details about title of first name to take into account)'])\n",
    "            top_full.append(main_data.iloc[Q[x]]['full'])\n",
    "            top_names.append(main_data.iloc[Q[x]]['Nom du tiers_First name'])\n",
    "            break\n",
    "\n",
    "scores=[]\n",
    "for text in top_full:\n",
    "    score=isQualified(query,[text,'sample text'],0)[0]\n",
    "    scores.append(score)\n",
    "\n",
    "cols=[]\n",
    "cols_count=0\n",
    "for s in scores:\n",
    "    cols_count+=1\n",
    "    cols.append(cols_count)\n",
    "\n",
    "if len(cols)>0:\n",
    "    max_score=max(scores)\n",
    "    scoresdf=pd.DataFrame(columns=cols)\n",
    "\n",
    "    scoresdf.loc[len(scoresdf)]=scores\n",
    "\n",
    "\n",
    "    nlargest = 5\n",
    "    topnlocs = np.argsort(-scoresdf.values, axis=1)[:, 0:nlargest]\n",
    "\n",
    "    for locs in topnlocs[0]:\n",
    "        extract_titles.append(top_titles[int(locs)])\n",
    "        extract_des.append(top_des[int(locs)])\n",
    "        extract_names.append(top_names[int(locs)])\n",
    "\n",
    "    results.loc[len(results)]=[query,extract_names,extract_titles,extract_des,max_score]\n",
    "            \n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\"columns\":[\"query\",\"top_names\",\"top_titles\",\"top_descriptions\",\"score\"],\"index\":[],\"data\":[]}\n"
     ]
    }
   ],
   "source": [
    "#To convert dataframe into json\n",
    "import json\n",
    "df_json=results.to_json(orient=\"split\")\n",
    "print(df_json)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## code to convert dataframe into excel\n",
    "\n",
    "import nltk\n",
    "import pandas as pd\n",
    "from odf import text, teletype\n",
    "from odf.opendocument import load\n",
    "from openpyxl import load_workbook\n",
    "\n",
    "def append_df_to_excel(filename, df, sheet_name='Sheet1', startrow=None,truncate_sheet=False,**to_excel_kwargs):\n",
    "\n",
    "    # ignore [engine] parameter if it was passed\n",
    "    if 'engine' in to_excel_kwargs:\n",
    "        to_excel_kwargs.pop('engine')\n",
    "\n",
    "    writer = pd.ExcelWriter(filename, engine='openpyxl')\n",
    "\n",
    "    # Python 2.x: define [FileNotFoundError] exception if it doesn't exist \n",
    "    try:\n",
    "        FileNotFoundError\n",
    "    except NameError:\n",
    "        FileNotFoundError = IOError\n",
    "\n",
    "\n",
    "    try:\n",
    "        # try to open an existing workbook\n",
    "        writer.book = load_workbook(filename)\n",
    "\n",
    "        # get the last row in the existing Excel sheet\n",
    "        # if it was not specified explicitly\n",
    "        if startrow is None and sheet_name in writer.book.sheetnames:\n",
    "            startrow = writer.book[sheet_name].max_row\n",
    "\n",
    "        # truncate sheet\n",
    "        if truncate_sheet and sheet_name in writer.book.sheetnames:\n",
    "            # index of [sheet_name] sheet\n",
    "            idx = writer.book.sheetnames.index(sheet_name)\n",
    "            # remove [sheet_name]\n",
    "            writer.book.remove(writer.book.worksheets[idx])\n",
    "            # create an empty sheet [sheet_name] using old index\n",
    "            writer.book.create_sheet(sheet_name, idx)\n",
    "\n",
    "        # copy existing sheets\n",
    "        writer.sheets = {ws.title:ws for ws in writer.book.worksheets}\n",
    "    except FileNotFoundError:\n",
    "        # file does not exist yet, we will create it\n",
    "        pass\n",
    "\n",
    "    if startrow is None:\n",
    "        startrow = 0\n",
    "\n",
    "    # write out the new sheet\n",
    "    df.to_excel(writer, sheet_name, startrow=startrow, **to_excel_kwargs)\n",
    "\n",
    "    # save the workbook\n",
    "    writer.save()\n",
    "    \n",
    "append_df_to_excel('PATH_TO_SAVE_THE_FILE/results_biased_description.xlsx',results)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
